# word vectors
  ## one-hot向量
  one-hot向量无法展示词语之间的相似性且向量维度过高，为词表维度
  ## 词向量（嵌入/神经词）
  - 通过词周围的上下文词可了解词本身的含义
  - 将词表示为一个密集的，更短的密集向量
  - 如果词之间有联系，它们会有某种相似的向量，对应于它们的点积较大
  - 可以将其视为高维空间中的向量，将每个词嵌入为高维空间中的一个位置，空间的维度就是向量的长度
  - 可以捕捉词的不同含义以及词与词之间相似的方式
  - 对一个词，要做的是学习一种方法，基于上下文中与之共现的其他词，将所有词表示为向量，并将它们嵌入到这个向量空间中
  - 语义空间：在这个空间中，具有相似含义的事物彼此接近
  - 每个词的嵌入为所有词义的平均值，最终会与由两种词义在语义上共同激发的词相似
  ## word2vec（学习词向量的方法）
  - 有一长串词的列表（语料库），每个词都将被向量所表示，遍历文本中的每个位置，每个位置对应一个列表，将有一个中心词（C）和它外部的词
  （o），然后使用C和外部词的词向量相似度来计算它们应该出现的概率与否（给定c的情况下出现o的概率）；持续调整词向量并且最大化概率
  - 希望的是在彼此附近上下文中出现的词的共现概率很高
  - 相当于一个不断滑动的以c为中心的窗口，每次计算c和周围所有词的相似度和这个词出现在c附近i位置的概率 
  - ![alt text](image-2.png)
  - 期望的是最大化这个似然，遍历文本中的每个位置，遍历上下文中的每个词，将它们相乘；这里采用负对数似然函数，最小化J就是最大化似然函数，目标函数J是平均负对数似然
  - ![alt text](image-3.png)
  - c 表示 center word / 中心词（输入词）
  - vc表示这个中心词c的 *输入词向量（input embedding）*，模型读到中心词 c 时，用来代表它的那根向量
  - o表示 outside word / 上下文词（要预测的目标词），表示这个上下文词 o 的 输出词向量（output embedding）你可以理解为：模型在“输出端/被预测端”用来代表词 o 的那根向量
  - P(o|c)表示在训练语料里，当中心词是 c 时，它的上下文窗口里出现词 o 的概率（由模型给出的预测分布）。
  - 视为一个优化问题，我们有大量的文本，我们希望找到词向量，使得我们观察到的文本中词的上下文概率尽量地大；所以将为每个词从随机向量开始，需要调整这些向量，使得上下文中词的计算概率增加；持续调整直到它不再上升，我们能得到我们能得到的最高概率估计（使用到梯度下降方法）
  - 对每个单词：
    - 当w为中心词时表示为v_w
    - 当w为上下文词时表示为u_w
  - 本质是先假设出初始随机词向量，然后用梯度下降法拟合出正确的词向量
  - 全部且仅有的模型参数就是词汇表中每个词的词向量
  - 参数量：词表数 * 词向量维度 * 2
  - 特别的，只计算对中心词的偏导数 