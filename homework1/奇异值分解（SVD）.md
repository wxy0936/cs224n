# 奇异值分解
## 核心
- *把一个很大的共现矩阵，压缩成两个小矩阵（低维表示）*，而且这种压缩在最小二乘意义下是最好的。

## 共现矩阵
行：中心词
列：上下文词
Xij：词i的窗口里出现词j的次数（或加权次数）
目标：给每个词一个d维向量，用来表示它的语义

## SVD
- 任意矩阵都可以写成：*X = U∑(VT)*
- 三步变换
  - VT:把列空间换一套坐标系（旋转/换基）
  - ∑：在每个新坐标轴上做缩放（奇异值越大越重要）
  - U：再把结果换到行空间的另一套坐标系
- 解释
  - U：列向量两两正交，长度为1（正交基）
  - V：同样是正交基
  - ∑：对角矩阵，对角线是非负数∆1≥∆2≥...（奇异值）（∆理解为：这一条主方向有多重要，越大表示解释的共现结构越多）
- 为什么能降维：
  - 关键：只保留最大的k个奇异值
  - X’ = Uk∑k(VkT)，x‘是最好的rank-k近似（最小二乘意义下误差最小）
  - 把X看成有许多模式/主题方向叠加而成，X=∆1u1v1T+∆2u2v2T+。。。，第一项是最主要的共现模式，第二项是词主要的共现模式，保留前k项，就是保留最重要的k个“共现模式”，丢掉噪声和细枝末节
- 怎么与词向量连起来：
  - 要从矩阵因子分解里拿到每个词的低维向量，因为X’ = Uk∑k(VkT)
    - Uk的每一行对应一个中心词的k维表示，Vk的每一行对应一个上下文词的k维表示，∑表示每个维度的重要程度（缩放）
    - 经典做法之一是把缩放吸收进去：
      - 中心词向量：W=Uk∑k
      - 上下文词向量：C=Vk∑k（或者给∑加个二分之一的次方）
    - 这样就有X’≈W（CT），类似score（c,o）=v_cTu_o,本质都是用两个低维向量的内积去拟合词和词的关联程度